/*
 * La Trobe University - Distributed Deep Learning System
 * Copyright 2016 Matthias Langer (t3l@threelights.de)
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */

package edu.latrobe.blaze.modules

import edu.latrobe._
import edu.latrobe.blaze._

/**
  * Very similar to the normal cross entropy. But operates on probabilities that
  * are already in log space. Using this together with a LogSoftmax is
  * recommended.
  * Like for the normal CrossEntropyConstraint, a transient LogSoftmax preceding
  * this layer should is recommended.
  *
  * J(x_a) = -y_a x_a
  *
  *        ---            ---
  *        \              \
  * J(x) = /   J(x_i) = - /   y_i x_i
  *        ---            ---
  *         i              i
  *
  * d J(x_a)
  * -------- = -y_a
  *  d x_a
  *
  *
  * Proof:
  * ------
  *
  * x_a was generated by logSoftmax(...), for the sake of brevity we rewrite it
  * as softmax(x_a) here.
  *
  *                                  (      ---                    )
  * D softmax(x_a)                   (      \                      )
  * -------------- = logSoftmax(x_a) ( da - /   logSoftmax(x_i) di )
  *     D x_a                        (      ---                    )
  *                                  (       i                     )
  *
  *
  * Normal use case (see logSoftmax layer for derivative of logSoftmax).
  *
  *
  * Then:
  * -----
  *
  * D J(x_a | y_a)          D logSoftmax(x_a)
  * -------------- = -y_a * -----------------
  *     D x_a                    D x_a
  *
  *
  * Where:
  * ------
  *
  *                     ---
  * D logSoftmax(x_a)   \   d logSoftmax(x_a)
  * ----------------- = /   ----------------- di
  *     D x_a           ---       d x_i
  *                      i
  *
  *                          ---
  *                          \
  *                   = da - /   softmax(x_i) di
  *                          ---
  *                           i
  *
  *
  * So we expand the equation:
  *
  *                ---
  *  d J           \
  * ----- = -y_a - /   softmax(x_i) * -y_i
  * d x_a          ---
  *                 i
  *
  *
  * Log space magic:
  *
  *                              ---
  *                              \
  *        = -y_a - softmax(x_a) /    -y_i
  *                              ---
  *                               i
  *
  *                       ( ---     )
  *                       ( \       )
  *        = softmax(x_a) ( /   y_i ) - y_a
  *                       ( ---     )
  *                       (  i      )
  *
  * Assuming that the sum of all y's equals to 1 (as it should be for softmax!)
  *
  *       = softmax(x_a) - y_a
  *
  */
final class ClassNLLConstraint(override val builder:        ClassNLLConstraintBuilder,
                               override val inputHints:     BuildHints,
                               override val seed:           InstanceSeed,
                               override val weightBufferBuilder: ValueTensorBufferBuilder)
  extends ConstraintEx[ClassNLLConstraintBuilder] {

  // ---------------------------------------------------------------------------
  //    Cost/Gradient computation related.
  // ---------------------------------------------------------------------------
  override protected def doEvaluate(reference:   Tensor,
                                    output:      Tensor,
                                    scaleFactor: Real)
  : Real = {
    val y = reference.valuesMatrixEx
    val x = output.valuesMatrix
    require(y.rows == x.rows && y.cols == x.cols)

    // TODO: Could make this faster by adding special version like fastFoldLeftActiveEx or so.
    val sum = MatrixEx.foldLeftEx(0.0, x, y)(
      (sum, x, y) => sum - x * y,
      (sum, x)    => sum
    )

    Real(sum * scaleFactor)
  }


  // ---------------------------------------------------------------------------
  //    Back propagation related.
  // ---------------------------------------------------------------------------
  override protected def doDeriveInputError(reference:   Tensor,
                                            output:      Tensor,
                                            scaleFactor: Real,
                                            error:       Tensor)
  : Tensor = {
    error.add(reference, -scaleFactor)

    /*
    Now in LogSoftmax code!
    val err  = error.valuesMatrix
    val yHat = output.valuesMatrix
    MatrixEx.transform(
      err,
      yHat
    )((err, yHat) => Real(err + Math.exp(yHat) * scaleFactor))
    RealArrayTensor.derive(error.layout.size, err)
    */

    error
  }

}

final class ClassNLLConstraintBuilder
  extends ConstraintExBuilder[ClassNLLConstraintBuilder] {

  override def repr
  : ClassNLLConstraintBuilder = this

  override def canEqual(that: Any)
  : Boolean = that.isInstanceOf[ClassNLLConstraintBuilder]

  override protected def doCopy()
  : ClassNLLConstraintBuilder = ClassNLLConstraintBuilder()

  override def build(hints:          BuildHints,
                     seed:           InstanceSeed,
                     weightsBuilder: ValueTensorBufferBuilder)
  : ClassNLLConstraint = new ClassNLLConstraint(
    this, hints, seed, weightsBuilder
  )

}

object ClassNLLConstraintBuilder {

  final def apply()
  : ClassNLLConstraintBuilder = new ClassNLLConstraintBuilder()

}