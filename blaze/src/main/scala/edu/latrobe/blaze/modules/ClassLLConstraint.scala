/*
 * La Trobe University - Distributed Deep Learning System
 * Copyright 2014 Matthias Langer (t3l@threelights.de)
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package edu.latrobe.blaze.modules

import edu.latrobe._
import edu.latrobe.blaze._
import scala.util.hashing._

/**
  * Softmax regression. All inputs must be valid probabilities.
  *
  * Best use together with a transient softmax activation function to see how it
  * works.
  *
  * Reminder: Breeze softmax returns log softmax directly.
  *           log(probabilities) = log(exp(a) / exp(softmax(a))) = a - softmax(a)
  *
  * f(x_a) = -y_a log(x_a + e)
  *
  *        ---            ---
  *        \              \
  * J(x) = /   J(x_i) = - /   y_i log(x_i + e)
  *        ---            ---
  *         i              i
  *
  * d J(x_a)    -y_a
  * -------- = -------
  *  d x_a     x_a + e
  *
  *
  * Proof:
  * ------
  *
  * Assuming x_a was generated by softmax(...), for the sake of brevity we
  * rewrite it as softmax(x_a) here.
  *
  *                               (      ---                 )
  * D softmax(x_a)                (      \                   )
  * -------------- = softmax(x_a) ( da - /   softmax(x_i) di )
  *     D x_a                     (      ---                 )
  *                               (       i                  )
  *
  * Normal use case (see softmax layer for derivative of softmax).
  *
  *
  * Then:
  * -----
  *
  * D J(x_a | y_a)        -y_a          D (softmax(x_a) + e)
  * -------------- = ---------------- * --------------------
  *     D x_a        softmax(x_a) + e          D x_a
  *
  *                       -y_a          D softmax(x_a)
  *                = ---------------- * --------------
  *                  softmax(x_a) + e       D x_a
  *
  *
  * Where:
  * ------
  *
  *                  ---
  * D softmax(x_a)   \   d softmax(x_a)
  * -------------- = /   -------------- di
  *     D x_a        ---      d x_i
  *                   i
  *
  *                                                 ---
  *                                                 \
  *                = softmax(x_a) da - softmax(x_a) /   softmax(x_i) di
  *                                                 ---
  *                                                  i
  *
  * So we expand the equation:
  *
  *                                                      ---
  *  d J                       -y_a                      \                      -y_i
  * ----- = softmax(x_a) ---------------- - softmax(x_a) /   softmax(x_i) ----------------
  * d x_a                softmax(x_a) + e                ---              softmax(x_i) + e
  *                                                       i
  *
  * Assuming that e = 0 (ideally):
  *
  *                                                  ---
  *                         -y_a                     \                   -y_i
  *       = softmax(x_a) ------------ - softmax(x_a) /   softmax(x_i) ------------
  *                      softmax(x_a)                ---              softmax(x_i)
  *                                                   i
  *
  *                             ---
  *                             \
  *       = -y_a - softmax(x_a) /   -y_i
  *                             ---
  *                               i
  *
  *                      ( ---     )
  *                      ( \       )
  *       = softmax(x_a) ( /   y_i ) - y_a
  *                      ( ---     )
  *                      (  i      )
  *
  * Assuming that the sum of all y's equals to 1 (as it should be for softmax!)
  *
  *       = softmax(x_a) - y_a
  *
  */
final class ClassLLConstraint(override val builder:        ClassLLConstraintBuilder,
                              override val inputHints:     BuildHints,
                              override val seed:           InstanceSeed,
                              override val weightBufferBuilder: ValueTensorBufferBuilder)
  extends ConstraintEx[ClassLLConstraintBuilder] {

  val epsilon
  : Real = builder.epsilon


  // ---------------------------------------------------------------------------
  //    Forward propagation related.
  // ---------------------------------------------------------------------------
  override protected def doEvaluate(reference:   Tensor,
                                    output:      Tensor,
                                    scaleFactor: Real)
  : Real = {
    val y = reference.valuesMatrixEx
    val x = output.valuesMatrix
    val e = DoubleEx(epsilon)

    // TODO: Could make this faster by adding special version like fastFoldLeftActiveEx or so.
    val sum = MatrixEx.foldLeftEx(0.0, x, y)(
      (sum, x, y) => {
        //val tmp: Double = if (yHat > e) DoubleEx(yHat) else e
        //sum - Math.log(tmp) * y
        sum - Math.log(x + e) * y
      },
      (sum, yHat) => sum
    )

    Real(sum * scaleFactor)
  }


  // ---------------------------------------------------------------------------
  //    Back propagation related.
  // ---------------------------------------------------------------------------
  override protected def doDeriveInputError(reference:   Tensor,
                                            output:      Tensor,
                                            scaleFactor: Real,
                                            error:       Tensor)
  : Tensor = {
    // error.add(reference, -scaleFactor)
    // error.add(output,    +scaleFactor)
    using(output.createSibling())(
      tmp => {
        tmp.set(reference, -Real.one)
        tmp.divide(output, epsilon)
        error.add(tmp, scaleFactor)
      }
    )
    error
  }

}

final class ClassLLConstraintBuilder
  extends ConstraintExBuilder[ClassLLConstraintBuilder] {

  override def repr
  : ClassLLConstraintBuilder = this

  private var _epsilon
  : Real = Real.epsilon

  def epsilon
  : Real = _epsilon

  def epsilon_=(value: Real)
  : Unit = {
    require(value >= Real.zero)
    _epsilon = value
  }

  def setEpsilon(value: Real)
  : ClassLLConstraintBuilder = {
    epsilon_=(value)
    this
  }

  override protected def doToString()
  : List[Any] = f"${_epsilon}%.4g" :: super.doToString()

  override def hashCode()
  : Int = MurmurHash3.mix(super.hashCode(), _epsilon.hashCode())

  override def canEqual(that: Any)
  : Boolean = that.isInstanceOf[ClassLLConstraintBuilder]

  override protected def doEquals(other: Equatable)
  : Boolean = super.doEquals(other) && (other match {
    case other: ClassLLConstraintBuilder =>
      _epsilon == other._epsilon
    case _ =>
      false
  })

  override protected def doCopy()
  : ClassLLConstraintBuilder = ClassLLConstraintBuilder()

  override def copyTo(other: InstanceBuilder)
  : Unit = {
    super.copyTo(other)
    other match {
      case other: ClassLLConstraintBuilder =>
        other._epsilon = _epsilon
      case _ =>
    }
  }


  // ---------------------------------------------------------------------------
  //    Weights and binding related.
  // ---------------------------------------------------------------------------
  override def build(hints:          BuildHints,
                     seed:           InstanceSeed,
                     weightsBuilder: ValueTensorBufferBuilder)
  : ClassLLConstraint = new ClassLLConstraint(this, hints, seed, weightsBuilder)

}

object ClassLLConstraintBuilder {

  final def apply()
  : ClassLLConstraintBuilder = new ClassLLConstraintBuilder

  final def apply(epsilon: Real)
  : ClassLLConstraintBuilder = apply().setEpsilon(epsilon)

}
